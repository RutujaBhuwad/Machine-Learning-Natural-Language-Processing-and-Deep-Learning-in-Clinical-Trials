Text Summerization:

Once the data frame of relevant text is ready, we begin the first phase of our research by summarising the text, which will subsequently be utilized for prediction. 
First, we applied the longformer model to summarise. 
Using Transformers for lengthy texts was a difficult task but the longformer made it easy. 
longformer introduced a sparse attention mechanism which reduces the computational time and memory requirements. Longformer is a BERT-like model built from the RoBERTa checkpoint and pre-trained on lengthy texts for MLM (multilevel modelling). 
It can handle sequences up to 4,096 characters long. 
Transformer-Based Models used to scale quadratically with the sequence length whereas Longformer scales linearly with sequence length.

We employed the Longformer2RoBERTa architecture, which uses Longformer as the encoder and RoBERTa as the decoder. 
Two classes are required, LongformerTokenizer class for tokenization, while EncoderDecoderModel class is required for Seq2Seq. 
We have used the "patrickvonplaten/longformer2roberta-cnn-dailymail-fp16" instance for our model as it contains the Seq2Seq model. 
As longformer is used for the encoder segment, longformer tokenization can be used for it. 
For the longformer tokenizer we used ''allenai/longformer-base-4096" pretrained model. 
When the text from the data frame was passed to the tokenizer, it created input ids using PyTorch-based Tensors. 
These input ids were used by the model for generating the summary.Once the summary is ready, it is decoded into the readable form using the tokenizer again. 
For each page, a summary of 4 lines was created, which means,


Later, all of the summaries were combined into one. As a result, a single summary of a single trial is created.

Reference:

Longformer:
HuggingFace - https://huggingface.co/transformers/v3.0.2/model_doc/longformer.html
